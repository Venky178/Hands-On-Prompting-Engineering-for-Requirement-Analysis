{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Consistency Prompting\n",
    "\n",
    "One of the more advanced techniques in prompt engineering is self-consistency, introduced by `Wang et al. (2022)`. \n",
    "\n",
    "This method seeks to improve upon the traditional greedy decoding typically used in chain-of-thought (CoT) prompting. \n",
    "\n",
    "The core concept involves sampling multiple diverse reasoning paths through few-shot CoT and leveraging these variations to determine the most consistent answer. The technique  enhances the effectiveness of CoT prompting, particularly for tasks requiring arithmetic and commonsense reasoning.\n",
    "\n",
    "## References:\n",
    "* [Wang et al. (2022)](https://arxiv.org/abs/2203.11171)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running this code on MyBind.org\n",
    "\n",
    "Note: remember that you will need to **adjust CONFIG** with **proper URL and API_KEY**!\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/GenILab-FAU/prompt-eng/HEAD?urlpath=%2Fdoc%2Ftree%2Fprompt-eng%2Fself_consistency.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llama3.2:latest', 'prompt': \"\\nProvide a requirement analysis for building a nutrition and fitness bot that uses Self-Consistency. The bot should generate multiple responses for the same query and assess which answer is the most consistent. It should weigh the responses based on factors such as scientific accuracy, user preferences, and relevance to the user's goal. For example:\\n\\nUser asks: 'What’s the best diet for weight loss?'\\nThe bot generates multiple responses, including options like keto, intermittent fasting, and calorie-counting.\\nThe bot evaluates which response is most consistent with the user’s goal and provides a final, accurate recommendation.\", 'stream': False, 'options': {'temperature': 1.0, 'num_ctx': 100, 'num_predict': 100}}\n",
      "That's not exactly how I envisioned the scenario, but I'll play along!\n",
      "\n",
      "So, you want to design a conversational AI system that can provide a recommended answer for weight loss, considering multiple options. Here's a more detailed breakdown:\n",
      "\n",
      "**Goals:**\n",
      "\n",
      "1. Provide a relevant and helpful response to users' queries about weight loss.\n",
      "2. Incorporate multiple potential solutions (e.g., diets, exercises, lifestyle changes) into the recommendation.\n",
      "3. Offer guidance on how to implement these solutions\n",
      "Time taken: 17.279s\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## \n",
    "##\n",
    "\n",
    "from _pipeline import create_payload, model_req\n",
    "\n",
    "#### (1) Adjust the inbounding Prompt, simulating inbounding requests from users or other systems\n",
    "\n",
    "\n",
    "#### (2) Apply the Zero-Shot Prompting Technique  \n",
    "PROMPT = \\\n",
    "f\"\"\"\n",
    "Provide a requirement analysis for building a nutrition and fitness bot that uses Self-Consistency. The bot should generate multiple responses for the same query and assess which answer is the most consistent. It should weigh the responses based on factors such as scientific accuracy, user preferences, and relevance to the user's goal. For example:\n",
    "\n",
    "User asks: 'What’s the best diet for weight loss?'\n",
    "The bot generates multiple responses, including options like keto, intermittent fasting, and calorie-counting.\n",
    "The bot evaluates which response is most consistent with the user’s goal and provides a final, accurate recommendation.\"\"\"\n",
    "\n",
    "#### (3) Configure the Model request, simulating Workflow Orchestration\n",
    "# Documentation: https://github.com/ollama/ollama/blob/main/docs/api.md\n",
    "payload = create_payload(target=\"ollama\",\n",
    "                         model=\"llama3.2:latest\", \n",
    "                         prompt=PROMPT, \n",
    "                         temperature=1.0, \n",
    "                         num_ctx=100, \n",
    "                         num_predict=100)\n",
    "\n",
    "### YOU DONT NEED TO CONFIGURE ANYTHING ELSE FROM THIS POINT\n",
    "# Send out to the model\n",
    "time, response = model_req(payload=payload)\n",
    "print(response)\n",
    "if time: print(f'Time taken: {time}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
